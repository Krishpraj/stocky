from flask import Flask, jsonify, request
from flask_cors import CORS
import requests
from textblob import TextBlob
import re
from dateparser import parse
from collections import Counter
import pandas as pd
import heapq

app = Flask(__name__)
CORS(app)

# Use NEws API key
NEWS_API_KEY = ""
NEWS_API_URL = "https://newsapi.org/v2/everything"



def get_articles(ticker,num_articles=30):
    if not ticker:
        return jsonify({"error": "Stock symbol is required"}), 400

    params = {
        'q': ticker, 
        'apiKey': NEWS_API_KEY, 
        'sortBy': 'relevancy',
        'language': 'en',
        'pageSize': num_articles
    }
    response = requests.get(NEWS_API_URL, params=params)
    if response.status_code == 200:
        return response.json().get('articles', [])
    else:
        raise Exception(f"Failed to retrieve articles: {response.status_code} {response.text}")
    




def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

def extract_dates(text):
    dates = re.findall(r'\d{1,2}[-/]\d{1,2}[-/]\d{2,4}', text)
    if not dates:
        dates = re.findall(r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\w*\s?\d{1,2},?\s?\d{4}\b', text)
    return [parse(date) for date in dates if parse(date) is not None]

def extract_keywords(text):
    words = re.findall(r'\b\w+\b', text.lower())
    return [word for word in words if len(word) > 3]

def summarize_article(content, num_sentences=3):
    sentences = content.split('.')
    sentences = [sentence.strip() for sentence in sentences if sentence]
    sentence_scores = {}
    keywords = extract_keywords(content)
    keyword_frequency = Counter(keywords)
    for sentence in sentences:
        score = sum(keyword_frequency.get(word.lower(), 0) for word in sentence.split())
        sentence_scores[sentence] = score
    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)
    return ' '.join(summary_sentences)


def summarize_company_activities(all_articles_content):
    combined_content = " ".join(all_articles_content)
    return summarize_article(combined_content, num_sentences=5)

def process_articles(ticker):
    articles = get_articles(ticker)
    processed_data = []
    all_keywords = []
    all_sentiments = []
    all_dates = []
    all_articles_content = []

    for article in articles:
        title = article.get('title')
        description = article.get('description')
        content = article.get('content')
        urlToImage = article.get('url')
        author = article.get('author')
        publishedAt = article.get('publishedAt')
        if content:
            sentiment = analyze_sentiment(content)
            keywords = extract_keywords(content)
            dates = extract_dates(content)
            processed_data.append({
                'Title': title,
                'Sentiment': sentiment,
                'Dates': publishedAt,
                'Description': description,
                'Content': content,
                'urlToImage': urlToImage,
                'author': author
            })
            all_keywords.extend(keywords)
            all_sentiments.append(sentiment)
            all_dates.extend(dates)
            all_articles_content.append(content)

    df = pd.DataFrame(processed_data)
    sentiment_avg = sum(all_sentiments) / len(all_sentiments) if all_sentiments else 0
    common_keywords = [item[0] for item in Counter(all_keywords).most_common(10)]
    common_dates = [str(date.date()) for date in Counter(all_dates).most_common(3)]
    company_summary = summarize_company_activities(all_articles_content)
    return df, sentiment_avg, common_keywords, common_dates, company_summary

@app.route('/api/news-analysis', methods=['GET'])
def news_analysis():
    ticker = request.args.get('ticker')
    if not ticker:
        return jsonify({"error": "Please provide a 'ticker' as a query parameter."}), 400
    try:
        df_articles, sentiment_avg, common_keywords, common_dates, company_summary = process_articles(ticker)
        sentiment_label = 'Neutral'
        if sentiment_avg > 0.1:
            sentiment_label = 'Positive'
        elif sentiment_avg < -0.1:
            sentiment_label = 'Negative'
        response = {
            "ticker": ticker,
            "sentiment": sentiment_label,
            "average_sentiment": sentiment_avg,
            "trending_keywords": common_keywords,
            "important_dates": common_dates,
            "company_summary": company_summary,
            "articles": df_articles.to_dict(orient='records')
        }
        return jsonify(response)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(port=5001)
